#coding=utf8
import random
import argparse
import time
import lasagne
import numpy as np
import theano
import theano.tensor as T
from sklearn.preprocessing import LabelBinarizer,label_binarize

def action_to_vector(x, n_classes): #x是bs*path_length
    result = np.zeros([x.shape[0], x.shape[1], n_classes])
    for i in range(x.shape[0]):
        for j in range(x.shape[1]):
            result[i,j]=label_binarize([int(x[i,j])],range(n_classes))[0]
    return result

def reward_count(total_reward, length, discout=0.99):
    discout_list = np.zeros(length)
    for idx in range(discout_list.shape[0]):
        step = discout ** idx
        discout_list[idx] = step
    result = np.dot(total_reward, discout_list)
    return result

def get_dataset(dimention=10,path_length=10):
    order_list=[[],[],[],[],[],[],[],[],[],[]]
    x=np.random.random((10000,dimention))
    y=np.zeros((10000))
    for i in (x):
        y=int(i[0]/0.1)
        order_list[y].append(i)

    total_roads=1000
    # path_length=10
    total_label=3
    final_x=np.zeros((total_roads,path_length,dimention))
    final_y=np.zeros((total_roads,path_length))
    for one_sample in range(total_roads):
        label_list=random.sample(range(dimention),total_label)
        one_shoot_label=label_list[-1]
        insert_idx=np.random.randint(0,path_length-1)
        final_x[one_sample,insert_idx]=random.sample(order_list[one_shoot_label],1)[0]
        final_y[one_sample,insert_idx]=one_shoot_label
        final_x[one_sample,-1]=random.sample(order_list[one_shoot_label],1)[0]
        final_y[one_sample,-1]=one_shoot_label
        for i in range(path_length-1):
            if i!=insert_idx:
                label=np.random.choice(label_list[:-1])
                final_y[one_sample,i]=label
                final_x[one_sample,i,:]=random.sample(order_list[label],1)[0]
    return final_x,np.int32(final_y)

class BatchedDotLayer(lasagne.layers.MergeLayer):

    def __init__(self, incomings, **kwargs):
        super(BatchedDotLayer, self).__init__(incomings, **kwargs)
        if len(incomings) != 2:
            raise NotImplementedError

    def get_output_shape_for(self, input_shapes):
        return (input_shapes[1][0], input_shapes[1][2])

    def get_output_for(self, inputs, **kwargs):
        return T.batched_dot(inputs[0], inputs[1])


class Model:
    def __init__(self,dimension=50,n_classes=10,batch_size=32,n_epoch=50,path_length=10,
                 n_paths=1000,max_norm=50,lr=0.05,discount=0.99,update_method='sgd',std=0.5,**kwargs):
        self.dimension=dimension
        self.n_classes=n_classes
        self.n_slot=n_classes
        self.batch_size=batch_size
        self.n_epoch=n_epoch
        self.path_length=path_length
        self.n_paths=n_paths
        self.max_norm=max_norm
        self.lr=lr
        self.std=std
        self.discount=discount
        self.x_dim=10

        if update_method=='sgd':
            self.update_method=lasagne.updates.sgd
        elif update_method=='adagrad':
            self.update_method=lasagne.updates.adagrad
        elif update_method=='adadelta':
            self.update_method=lasagne.updates.adadelta
        elif update_method=='rmsprop':
            self.update_method=lasagne.updates.rmsprop
        self.build()

    def build(self):
        x_range=T.tensor3()
        x_action=T.tensor3()
        x_reward=T.vector()
        x_memory=T.tensor4()

        self.x_range_shared=theano.shared(np.zeros((self.batch_size,self.path_length,self.x_dim),dtype=theano.config.floatX),borrow=True)
        self.x_range_action=theano.shared(np.zeros((self.batch_size,self.path_length,self.n_classes),dtype=theano.config.floatX),borrow=True)
        self.x_range_reward=theano.shared(np.zeros(self.batch_size,dtype=theano.config.floatX),borrow=True)
        self.x_range_memory=theano.shared(np.zeros((self.batch_size,self.path_length,self.n_classes,self.x_dim),dtype=theano.config.floatX),borrow=True)

        '''前期的框架模型，主要是得到x到h的映射，以及memory的构建'''
        D1, D2, D3 = lasagne.init.Normal(std=self.std), lasagne.init.Normal(std=self.std), lasagne.init.Normal(std=self.std)
        l_range_in = lasagne.layers.InputLayer(shape=(self.batch_size,self.path_length,self.x_dim))
        l_range_flatten = lasagne.layers.ReshapeLayer(l_range_in,[self.batch_size*self.path_length,self.x_dim])
        # l_range_dense1 = lasagne.layers.DenseLayer(l_range_flatten,self.dimension,W=D1,nonlinearity=lasagne.nonlinearities.sigmoid)
        l_range_dense2 = lasagne.layers.DenseLayer(l_range_flatten,self.x_dim,W=D2,nonlinearity=lasagne.nonlinearities.sigmoid) #[bs*path_length,dimension]
        l_range_dense2 = lasagne.layers.DenseLayer(l_range_dense2,self.x_dim,W=D2,nonlinearity=lasagne.nonlinearities.sigmoid) #[bs*path_length,dimension]
        l_range_dense2_origin=lasagne.layers.ReshapeLayer(l_range_dense2,[self.batch_size,self.path_length,self.x_dim])
        l_range_hidden=lasagne.layers.ReshapeLayer(l_range_dense2,[self.batch_size*self.path_length,1,self.x_dim])
        # l_range_mu = lasagne.layers.DenseLayer(l_range_hidden,self.n_slot,W=D2,nonlinearity=lasagne.nonlinearities.softmax)

        '''Policy Gradient Methods的模型，主要是从Memory状态得到action的概率'''
        l_range_memory_in = lasagne.layers.InputLayer(shape=(self.batch_size,self.path_length,self.n_classes,self.x_dim))
        l_range_memory = lasagne.layers.ReshapeLayer(l_range_memory_in,[self.batch_size*self.path_length,self.n_classes,self.x_dim])
        if 0:
            l_range_status=lasagne.layers.ConcatLayer((l_range_memory,l_range_hidden),axis=1) #[bs*pl,(n_class+1),dim]
            l_range_mu=lasagne.layers.DenseLayer(l_range_status,self.n_classes,W=D3,nonlinearity=lasagne.nonlinearities.sigmoid)
            l_range_mu=lasagne.layers.DenseLayer(l_range_mu,self.n_classes,W=D3,nonlinearity=lasagne.nonlinearities.softmax)
            l_range_mu = lasagne.layers.ReshapeLayer(l_range_mu,[self.batch_size,self.path_length,self.n_classes])
        if 1:
            l_range_status=BatchedDotLayer((l_range_memory,l_range_hidden)) #[bs*pl,(n_class+1),dim]
            l_range_mu=lasagne.layers.NonlinearityLayer(l_range_status,self.n_classes,nonlinearity=lasagne.nonlinearities.softmax)
            l_range_mu = lasagne.layers.ReshapeLayer(l_range_mu,[self.batch_size,self.path_length,self.n_classes])

        '''模型的总体参数和更新策略等'''
        hidden = lasagne.layers.helper.get_output(l_range_dense2_origin, {l_range_in: x_range})
        probas_range = lasagne.layers.helper.get_output(l_range_mu, {l_range_in: x_range,l_range_memory_in:x_memory})
        params=lasagne.layers.helper.get_all_params(l_range_mu,trainable=True)
        givens = {
            x_range: self.x_range_shared,
            x_action: self.x_range_action,
            x_reward: self.x_range_reward,
            x_memory: self.x_range_memory
        }
        cost=-T.mean(T.sum(T.sum(T.log(probas_range)*x_action,axis=2),axis=1)*x_reward)
        grads=T.grad(cost,params)
        scaled_grads = lasagne.updates.total_norm_constraint(grads, self.max_norm)
        updates = self.update_method(scaled_grads, params, learning_rate=self.lr)

        self.output_model_range = theano.function([],[probas_range,cost,hidden],givens=givens,on_unused_input='ignore',allow_input_downcast=True)
        self.output_model_range_updates = theano.function([],[probas_range,cost,hidden],updates=updates,givens=givens,on_unused_input='ignore',allow_input_downcast=True)
        self.output_hidden = theano.function([x_range],[hidden[:,0]],on_unused_input='ignore',allow_input_downcast=True)
        # self.output_model_range = theano.function([],[probas_range,cost],givens=givens,updates=updates,on_unused_input='ignore',allow_input_downcast=True)

    def sample_one_path(self,now_state,now_memory_label, prob,this_label,idx_path_length):
        new_memory_state=np.zeros([self.n_classes,self.x_dim])
        new_memory_label=np.zeros([self.n_classes,self.path_length])
        reward=0.0

        now_memory_label_count=np.int32(now_memory_label!=-1).sum(axis=1)#计算当前memory每个slot各有多少个数据存储
        action=np.random.choice(range(self.n_classes),p=prob)
        index_to_insert=int(np.argwhere(now_memory_label[action]==-1)[0])
        #定义reward
        if now_memory_label[action][0]==-1:#证明是新开的一个slot
            reward=-1
        else: #证明action是一个已经有label的slot位置
            label_count_dict={}
            for jj in (now_memory_label[action]):
                if jj==-1:
                    break
                if jj in label_count_dict.keys():
                    label_count_dict[jj]+=1
                else:
                    label_count_dict[jj]=1
            max_num=max(label_count_dict.values())
            if this_label in label_count_dict and label_count_dict[this_label]==max_num: #存在了label重合最多的slot里
                if idx_path_length==self.path_length-1 and index_to_insert==1:#如果是一个样本的最后一步
                    reward=200
                    print '200~~~~!\n'
                else:
                    reward=3
            elif this_label not in label_count_dict:#存在了一个有label的但是不是这个label的slot，给予比较大的惩罚
                reward=-5
            else:#剩下的是存在一个已经有这个label的slot里，但是这个label不是最多的
                reward=-2
        #更新状态，加权平均，更新label记录
        now_state[action]=now_state[-1]+(now_state[action]*now_memory_label_count[action])
        now_state[action]=now_state[action]/(now_memory_label_count[action]+1)
        new_memory_state=now_state[:-1]
        now_memory_label[action][index_to_insert]=this_label
        new_memory_label=now_memory_label


        return action,new_memory_state,reward,new_memory_label

    def train(self):
        batch_size=self.batch_size
        n_paths=self.n_paths
        path_length=self.path_length
        dimention=self.x_dim
        n_classes=self.n_classes
        xx, yy = get_dataset(dimention,path_length) #xx是[sample,path_length,dimension]，yy是[sample.path_length]
        for epoch in range(self.n_epoch):
            begin_time = time.time()
            batch_total_number = len(xx) / batch_size
            '''每一轮的shuffle策略'''
            zipp=zip(xx,yy)
            random.shuffle(zipp)
            xx=np.array([one[0] for one in zipp])
            yy=np.array([one[1] for one in zipp])

            for idx_batch in range(batch_total_number):#对于每一个batch
                # 初始化两个循环的参数，state和概率
                xx_batch = xx[idx_batch * batch_size:(idx_batch + 1) * batch_size]
                yy_batch = yy[idx_batch * batch_size:(idx_batch + 1) * batch_size]
                xx_batch_0=xx_batch[:,0,:].reshape([xx_batch.shape[0],1,xx_batch.shape[-1]])
                xx_batch_0_repeat=xx_batch_0.repeat(path_length,axis=1)
                memory_0=np.zeros((batch_size,self.n_classes,xx.shape[-1]))
                memory_0_repeat=np.zeros((batch_size,path_length,self.n_classes,xx.shape[-1]))

                tmp_cost, tmp_result, tmp_reward = 0, 0, 0
                for repeat_time in range(n_paths):  # 每一个batch都要经过多次重复采样获取不同的道路
                    # 但是第一步的初始化状态都是一样的
                    self.x_range_shared.set_value(xx_batch_0_repeat)
                    self.x_range_memory.set_value(memory_0_repeat)
                    probbb = self.output_model_range()[0]
                    probs_sample_0 = probbb[:, 0]

                    total_state = np.zeros([batch_size, path_length, n_classes+1,dimention])
                    total_memory_label=np.zeros([batch_size,path_length,n_classes,path_length],dtype=np.int32)-1 #取作-1，标志着还没有存放过样本
                    total_reward = np.zeros([batch_size, path_length])
                    total_action = np.zeros([batch_size, path_length])
                    total_probs = np.zeros([batch_size, path_length, n_classes])

                    total_state[:, 0, -1] =self.output_hidden(xx_batch_0_repeat)[0]
                    total_state[:, 0, :-1] = memory_0
                    '''state是[bs,class+1,x_dim]尺度的东西，第二个维度前class个是memory状态,最后一个是当前输入的隐层表达'''
                    total_probs[:, 0, :] = probs_sample_0

                    for t in range(path_length):  # 进行了path_length步
                        for idx, prob in enumerate(total_probs[:, t, :]):  # 对于batch里的每一个样本
                            now_state = total_state[idx, t].copy()
                            now_memory_label=total_memory_label[idx,t].copy()
                            action, new_memory_state, reward, new_memory_label = self.sample_one_path(now_state,now_memory_label, prob,yy_batch[idx,t],t)
                            # action, new_state, reward = sample_one_path_plus(now_state, prob)
                            # action是这一步采取的动作，state是进入的新的状态，prob四
                            total_action[idx, t] = action
                            total_reward[idx, t] = reward
                            # 更新state和概率,下一个循环使用
                            if t != path_length - 1:
                                # TODO:这里需要模型中添加一个只从x得到隐层表示的小函数
                                # new_h=self.output_hidden(xx_batch[idx,t+1])[-1]
                                # new_state=np.concatenate((new_memory_state,new_h),axis=1)
                                total_state[idx, t + 1,:-1] = new_memory_state
                                total_memory_label[idx,t+1]=new_memory_label
                            del new_memory_state, now_state,new_memory_label

                        if t != path_length - 1:
                            total_state[:, t + 1,-1,:] =self.output_hidden(xx_batch[:,t+1].reshape(batch_size,1,dimention).repeat(path_length,axis=1))[0]
                            self.x_range_shared.set_value(xx_batch[:,t+1].reshape(batch_size,1,dimention).repeat(path_length,axis=1))
                            self.x_range_memory.set_value(total_state[:,t+1,:-1,:].reshape(batch_size,1,n_classes,dimention).repeat(path_length,axis=1))
                            total_probs[:, t + 1, :] = self.output_model_range()[0][:,0]

                    self.x_range_shared.set_value(xx_batch)
                    self.x_range_action.set_value(action_to_vector(total_action, n_classes))
                    self.x_range_reward.set_value(reward_count(total_reward, length=path_length, discout=self.discount))
                    aver_reward = np.mean(np.sum(np.float32(total_reward), axis=1))
                    espect_reward = np.mean(np.float32(reward_count(total_reward,path_length,discout=self.discount)), axis=0)
                    _, cost = self.output_model_range_updates()[:-1]
                    tmp_cost += cost
                    tmp_result += aver_reward
                    tmp_reward += espect_reward
                    print 'cost:{},average_reward:{}'.format(cost,aver_reward)
                    print total_state[0][-1]
                    print total_action[0]
                    print total_memory_label[0][-1]
                    print total_reward[0]
                    print _[0]
                    # print _[0]
                    print '\n\n\n'

                print 'cost:{},average_reward:{},espect_reward:{}'.format(tmp_cost / n_paths, tmp_result / n_paths, tmp_reward/ n_paths)

                if tmp_result / n_paths  > 0:
                    print total_state[0][-1]
                    print total_action[0]
                    print total_memory_label[0][-1]
                    print total_reward[0]
                    print _[0]
                    # print total_probs[0]
                    print '\n\n'

            print 'epoch:{},time:{}'.format(epoch, time.time() - begin_time)

if __name__=='__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--task', type=int, default=1, help='Task#')
    parser.add_argument('--dimension', type=int, default=20, help='Dimension#')
    parser.add_argument('--n_classes', type=int, default=10, help='Task#')
    parser.add_argument('--batch_size', type=int, default=1000, help='Task#')
    parser.add_argument('--n_epoch', type=int, default=50, help='Task#')
    parser.add_argument('--path_length', type=int, default=11, help='Task#')
    parser.add_argument('--n_paths', type=int, default=100, help='Task#')
    parser.add_argument('--max_norm', type=float, default=50, help='Task#')
    parser.add_argument('--lr', type=float, default=0.2, help='Task#')
    parser.add_argument('--discount', type=float, default=0.99, help='Task#')
    parser.add_argument('--std', type=float, default=1, help='Task#')
    parser.add_argument('--update_method', type=str, default='rmsprop', help='Task#')
    args=parser.parse_args()
    print '*' * 80
    print 'args:', args
    print '*' * 80
    model=Model(**args.__dict__)
    model.train()
